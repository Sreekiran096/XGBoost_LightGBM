{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "import cudf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Metadata and Climate Data from GHCND\n",
    "metadata = pd.read_fwf('https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt', header= None)\n",
    "metadata.columns = ['station_id', 'latitude', 'longitude', 'elevation', 'name', 'hcn_crn_flag', 'gsn_flag', 'wmo_id']\n",
    "metadata = metadata[['station_id', 'latitude', 'longitude', 'elevation', 'name']]\n",
    "metadata = cudf.DataFrame(metadata)\n",
    "\n",
    "temp_data = cudf.read_csv('/datasets/weather_decomp/2000.csv',header=None) #Downloaded from https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/\n",
    "temp_data.columns = ['station_id', 'date', 'type', 'value', 'mflag', 'qflag', 'sflag', 'unique_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "req_cols = ['station_id', 'latitude', 'longitude', 'elevation', 'date', 'name', 'type', 'sflag', 'value']\n",
    "data = cudf.merge(temp_data, metadata, on='station_id', how='left')\n",
    "data = data[req_cols]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Feature Extraction\n",
    "dates = cudf.to_datetime(data['date'].astype(str), format='%Y%m%d')\n",
    "dates = cudf.DatetimeIndex(dates)\n",
    "data['month'] = dates.month\n",
    "data['day'] = dates.day\n",
    "\n",
    "# Extract country code\n",
    "data['country_code'] = data['station_id'].astype(str).str[:2]\n",
    "\n",
    "# Delete metadata and climate data\n",
    "del temp_data\n",
    "del metadata\n",
    "\n",
    "# Feature Selection\n",
    "features = ['country_code', 'sflag', 'month', 'type', 'day', 'latitude', 'longitude', 'elevation']\n",
    "target = 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnverting dtypes to categorical\n",
    "data['country_code'] = data['country_code'].astype('category')\n",
    "data['month'] = data['month'].astype('category')\n",
    "data['day'] = data['day'].astype('category')\n",
    "data['type'] = data['type'].astype('category')\n",
    "data['sflag'] = data['sflag'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:08:21] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:08:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"n_estimators\", \"silent\", \"verbose_eval\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[04:08:28] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:08:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"n_estimators\", \"silent\", \"verbose_eval\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[04:08:37] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:08:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"n_estimators\", \"silent\", \"verbose_eval\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[04:08:57] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:08:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"n_estimators\", \"silent\", \"verbose_eval\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[04:09:31] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:09:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"n_estimators\", \"silent\", \"verbose_eval\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_params = {'objective': 'reg:linear', \n",
    "                'learning_rate': 0.1, \n",
    "                'colsample_bytree' : 0.3, \n",
    "                'max_depth': 5, \n",
    "                'n_estimators':10, \n",
    "                'alpha' : 10, \n",
    "                'silent': True, \n",
    "                'verbose_eval': True, \n",
    "                'tree_method':'gpu_hist'}\n",
    "\n",
    "# Create train and test dmatrix\n",
    "dtrain = xgb.DMatrix(data[features], data[target], enable_categorical=True)\n",
    "\n",
    "boost_rounds = [50, 100, 200, 350, 500]\n",
    "\n",
    "latency_dict ={key: list() for key in ['time', 'iterations', 'memory']}\n",
    "\n",
    "for k in boost_rounds:\n",
    "    model_params['n_estimators'] = k\n",
    "    t = time.time()\n",
    "\n",
    "    trained_model = xgb.train(model_params, dtrain, num_boost_round=k )   #, evals=[(dtrain, 'train')]\n",
    "    latency_dict['time'].append(time.time()-t)\n",
    "    latency_dict['iterations'].append(k)\n",
    "    latency_dict['memory'].append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "\n",
    "    del trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.13974928855896,\n",
       " 9.741299867630005,\n",
       " 19.455682277679443,\n",
       " 33.842817306518555,\n",
       " 48.2730872631073]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_dict['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage on 3.2 million records: 1830.99375MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Average memory usage on 3.2 million records: {}MB\".format(np.mean(latency_dict['memory'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme/1/svadaga/miniconda3/envs/cuml_dev/lib/python3.9/site-packages/lightgbm/engine.py:142: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/nvme/1/svadaga/miniconda3/envs/cuml_dev/lib/python3.9/site-packages/lightgbm/engine.py:142: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model_params = {'objective':'regression',\n",
    "                 'learning_rate': 0.1, \n",
    "                 'feature_fraction': 0.3, \n",
    "                 'max_depth': 5, \n",
    "                 'verbose': -1, \n",
    "                 'n_estimators':10, \n",
    "                 'device': 'gpu', \n",
    "                 'gpu_platform_id':'3', \n",
    "                 'gpu_device_id':'3'\n",
    "                 }\n",
    "\n",
    "lgb_train = lgb.Dataset(data[features].to_pandas(), data[target].to_pandas()) # LightGBM don't support cudf dataframes\n",
    "\n",
    "lgb_latency_dict ={key: list() for key in ['time','iterations','memory']}\n",
    "\n",
    "for k in boost_rounds:\n",
    "    model_params['n_estimators'] = k\n",
    "    t = time.time()\n",
    "\n",
    "    trained_model = lgb.train(model_params, lgb_train, num_boost_round = k)   #, evals=[(dtrain, 'train')]\n",
    "    lgb_latency_dict['time'].append(time.time()-t)\n",
    "    lgb_latency_dict['iterations'].append(k)\n",
    "    lgb_latency_dict['memory'].append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "\n",
    "    del trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.37099003791809,\n",
       " 29.78584051132202,\n",
       " 54.16922187805176,\n",
       " 98.2325119972229,\n",
       " 130.54339241981506]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_latency_dict['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage on 3.2 million records: 2364.42578125MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Average memory usage on 3.2 million records: {}MB\".format(np.mean(lgb_latency_dict['memory'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f75c3baf5102bfc8a4fbca324a6b2053f469a5b84d8bd770c9d4e4e0b05194ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
